---
NEP: 509
Title: Stateless validation Stage 0
Authors: Robin Cheng, Anton Puhach, Alex Logunov, Yoon Hong
Status: Draft
DiscussionsTo: https://docs.google.com/document/d/1C-w4FNeXl8ZMd_Z_YxOf30XA1JM6eMDp5Nf3N-zzNWU/edit?usp=sharing, https://docs.google.com/document/d/1TzMENFGYjwc2g5A3Yf4zilvBwuYJufsUQJwRjXGb9Xc/edit?usp=sharing
Type: Protocol
Version: 1.0.0
Created: 2023-09-19
LastUpdated: 2023-09-19
---

## Summary

The NEP proposes an solution to achieve phase 2 of sharding (where none of the validators needs to track all shards), with stateless validation, instead of the traditionally proposed approach of fraud proof and state rollback.

The fundamental idea is that validators do not need to have state locally to validate chunks. 

* Under stateless validation, the responsibility of a chunk producer extends to packaging transactions and receipts and annotating them with state witnesses. This extended role will be called "chunk proposers".
* The state witness of a chunk is defined to be a subset of the trie state, alongside its proof of inclusion in the trie, that is needed to execute a chunk. A state witness allows anyone to execute the chunk without having the state of its shard locally. 
* Then, at each block height, validators will be randomly assigned to a shard, to validate the state witness for that shard. Once a validator receives both a chunk and its state witness, it verifies the state transition of the chunk, signs a chunk endorsement and sends it to the block producer. This is similar to, but separate from, block approvals and consensus.
* The block producer waits for sufficient chunk endorsements before including a chunk into the block it produces, or omits the chunk if not enough endorsements arrive in time.

## Motivation

As phase 1 of sharding requires block producers to track all shards due to underlying security concerns, the team explored potential ways to achieve phase 2 of sharding, where none of the validators has to track all shards.

The early design of phase 2 relied on the security assumption that as long as there is one honest validator or fisherman tracking a shard, the shard is secure; by doing so, it naturally relied on protocol's ability to handle challenges (when an honest validator or fisherman detects a malicious behavior and submits a proof of such), state rollbacks (when validators agree that the submitted challenge is valid), and slashing (to punish the malicious validator). While it sounds straightforward and simple on paper, the complex interactions between these abilities and the rest of the protocol led to concrete designs that were extremely complicated, involving several specific problems we still don't know how to solve.

As a result, the team sought alternative approaches and concluded that stateless validation is the most realistic and promising one; the stateless validation approach does not assume the existence of a fishermen, does not rely on challenges, and never rolls back state. Instead, it relies on the assumption that a shard is secure if every single chunk in that shard is validated by a randomly sampled subset of all validators, to always produce valid chunks in the first place.

## Specification

### Assumptions

* Not more than 1/3 of validators is corrupted.
* In memory trie is enabled - [REF](https://docs.google.com/document/d/1_X2z6CZbIsL68PiFvyrasjRdvKA_uucyIaDURziiH2U/edit?usp=sharing)
* State sync is enabled (so that nodes can track different shards across epochs)
* Merkle Patricia Trie continues to be the state trie implementation
* Congestion Control is enabled - [NEP-539](https://github.com/near/NEPs/pull/539)

### Design requirements

* No validator needs to track all shards.
* Security of protocol must not degrade.
  * Validator assignment for both chunk validation and block validation should not create any security vulnerabilities.
* Block processing time should not take significantly more than what it takes today.
* Any additional load on network and compute should not negatively affect existing functionalities of any node in the blockchain.
  * The cost of additional network and compute should be acceptable.
* Validator rewards should not be reduced.

### Current design

The current high-level chunk production flow, excluding details and edge cases, is as follows:
* Block producer at height `H` `BP(H)` produces block `B(H)` with chunks accessible to it and distributes it.
* Chunk producer for shard `S` at height `H+1` `CP(S, H+1)` produces chunk `C(S, H+1)` based on `B(H)` and distributes it.
* `BP(H+1)` collects all chunks at height `H+1` until certain timeout is reached.
* `BP(H+1)` produces block `B(H+1)` with chunks `C(*, H+1)` accessible to it and distributes it, etc.

The "induction base" is at genesis height, where genesis block with default chunks is accessible to everyone, so chunk producers can start right away from genesis height + 1.

One can observe that there is no "chunk validation" step here. In fact, validity of chunks is implicitly guaranteed by **requirement for all block producers to track all shards**.
To achieve phase 2 of sharding, we want to drop this requirement. For that, we propose the following changes to the flow:
 
### New design

* Chunk producer, in addition to producing a chunk, produces new `ChunkStateWitness` message. The `ChunkStateWitness` contains data which is enough to prove validity of the chunk's header that is being produced.
  * `ChunkStateWitness` proves to anyone, including those who track only block data and no shards, that this chunk header is correct.
  * `ChunkStateWitness` is not part of the chunk itself; it is distributed separately and is considered transient data.
* The chunk producer distributes the `ChunkStateWitness` to a subset of **chunk validators** which are assigned for this shard. This is in addition to, and independent of, the existing chunk distribution logic (implemented by `ShardsManager`) today.
  * Chunk Validator selection and assignment are described below.
* A chunk validator, upon receiving a `ChunkStateWitness`, validates the state witness and determines if the chunk header is indeed correctly produced. If so, it sends a `ChunkEndorsement` to the current block producer.
* As the existing logic is today, the block producer for this block waits until either all chunks are ready, or a timeout occurs, and then proposes a block containing whatever chunks are ready. Now, the notion of readiness here is expanded to also having more than 2/3 of chunk endorsements by stake.
  * This means that if a chunk does not receive enough chunk endorsements by the timeout, it will not be included in the block. In other words, the block only contains chunks for which there is already a consensus of validity. **This is the key reason why we will no longer need fraud proofs / tracking all shards**.
  * The 2/3 fraction has the denominator being the total stake assigned to validate this shard, *not* the total stake of all validators.
* The block producer, when producing the block, additionally includes the chunk endorsements (at least 2/3 needed for each chunk) in the block's body. The validity of the block is expanded to also having valid 2/3 chunk endorsements by stake for each chunk included in the block.
  * If a block fails validation because of not having the required chunk endorsements, it is considered a block validation failure for the purpose of Doomslug consensus, just like any other block validation failure. In other words, nodes will not apply the block on top of their blockchain, and (block) validators will not endorse the block.

So the high-level specification can be described as the list of changes in the validator roles and responsibilities:

* Block producers:
  * (Same as today) Produce blocks, (new) including waiting for chunk endorsements
  * (Same as today) Maintain chunk parts (i.e. participates in data availability based on Reed-Solomon erasure encoding)
  * (New) No longer require tracking any shard
  * (Same as today) Should have high barrier of entry for security reasons, to make block double signing harder.
* Chunk producers:
  * (Same as today) Produce chunks
  * (New) Produces and distributes state witnesses to chunk validators
  * (Same as today) Must track the shard it produces the chunk for
* Block validators:
  * (Same as today) Validate blocks, (new) including verifying chunk endorsements
  * (Same as today) Vote for blocks with endorsement or skip messages
  * (New) No longer require tracking any shard
  * (Same as today) Must collectively have a majority of all the validator stake, for security reasons.
  * (Same as today) Should have high barrier of entry to keep `BlockHeader` size low, because it is proportional to the total byte size of block validator signatures;
* (New) Chunk validators:
  * Validate state witnesses and sends chunk endorsements to block producers
  * Do not require tracking any shard
  * Must collectively have a majority of all the validator stake, to ensure the security of chunk validation.

See the Validator Structure Change section below for more details.

### Out of scope

* Resharding support.
* Data size optimizations such as compression, for both chunk data and state witnesses, except basic optimizations that are practically necessary.
* Separation of consensus and execution, where consensus runs independently from execution, and validators asynchronously perform state transitions after the transactions are proposed on the consensus layer, for the purpose of amortizing the computation and network transfer time.
* ZK integration.
* Underlying data structure change (e.g. verkle tree).

## Reference Implementation

Here we carefully describe new structures and logic introduced, without going into too much technical details.

### Validator Structure Change

#### Roles
Currently, there are two different types of validators. Their responsibilities are defined as in the following pseudocode:

```python
if index(validator) < 100:
    roles(validator).append("block producer")
roles(validator).append("chunk producer")
```

The validators are ordered by non-increasing stake in the considered epoch. Here and below by "block production" we mean both production and validation.

With stateless validation, this structure must change for several reasons:
* Chunk production is the most resource consuming activity.
* (Only) chunk production needs state in memory while other responsibilities can be completed via acquiring state witness
* Chunk production does not have to be performed by all validators. 

Hence, to make transition seamless, we change the role of nodes out of top 100 to only validate chunks:

```python
if index(validator) < 100:
    roles(validator).append("chunk producer")
    roles(validator).append("block producer")
roles(validator).append("chunk validator")
```

The more stake validator has, the more **heavy** work it will get assigned, because we assume that validators with higher stakes have more powerful hardware. 
With stateless validation, relative heaviness of the work changes. Comparing to the current order "block production" > "chunk production", the new order is "chunk production" > "block production" > "chunk validation".

Shards are equally split among chunk producers: as in Mainnet on 12 Jun 2024 we have 6 shards, each shard would have ~16 chunk producers assigned.

In the future, with increase in number of shards, we can generalise the assignment by saying that each shard should have `X` chunk producers assigned, if we have at least `X * S` validators. In such case, pseudocode for the role assignment would look as follows:

```python
if index(validator) < X * S:
    roles(validator).append("chunk producer")
if index(validator) < 100:
    roles(validator).append("block producer")
roles(validator).append("chunk validator")
```

#### Rewards

Reward for each validator is defined as `total_epoch_reward * validator_relative_stake * work_quality_ratio`, where:
* `total_epoch_reward` is selected so that total inflation of the token is 5% per annum;
* `validator_relative_stake = validator_stake / total_epoch_stake`;
* `work_quality_ratio` is the measure of the work quality from 0 to 1.

So, the actual reward never exceeds total reward, and when everyone does perfect work, they are equal.
For the context of the NEP, it is enough to assume that `work_quality_ratio = avg_{role}({role}_quality_ratio)`.
So, if node is both a block and chunk producer, we compute quality for each role separately and then take average of them.
  
When epoch is finalized, all block headers in it uniquely determine who was expected to produce each block and chunk.
Thus, if we define quality ratio for block producer as `produced_blocks/expected_blocks`, everyone is able to compute it.
Similarly, `produced_chunks/expected_chunks` is a quality for chunk producer. 
It is more accurate to say `included_chunks/expected_chunks`, because inclusion of chunk in block is a final decision of a block producer which defines success here.

Ideally, we could compute quality for chunk validator as `produced_endorsements/expected_endorsements`. Unfortunately, we won't do it in Stage 0 because:
* Mask of endorsements is not part of the block header, and it would be a significant change;
* Block producer doesn't have to wait for all endorsements to be collected, so it could be unfair to say that endorsement was not produced if block producer just went ahead.

So for now we decided to compute quality for chunk validator as ratio of `included_chunks/expected_chunks`, where we iterate over chunks which node was expected to validate.
The obvious drawback here is that if chunks are not produced at all, chunk validators will also be impacted. We plan to address it in the future releases.
 
#### Kickouts

In addition to that, if node performance is too poor, we want a mechanism to kick it out of the validator list, to ensure healthy protocol performance and validator rotation.
Currently, we have a threshold for each role, and if for some role the same `{role}_quality_ratio` is lower than threshold, the node is kicked out.

If we write this in pseudocode,

```python
if validator is block producer and block_producer_quality_ratio < 0.8:
    kick out validator
if validator is chunk producer and chunk_producer_quality_ratio < 0.8:
    kick out validator
```

For chunk validator, we apply absolutely the same formula. However, because:
* the formula doesn't count endorsements explicitly 
* for chunk producers it kind of just makes chunk production condition stronger without adding value

we apply it to nodes which **only validate chunks**. So, we add this line:

```python
if validator is only chunk validator and chunk_validator_quality_ratio < 0.8:
    kick out validator
```

As we pointed out above, current formula `chunk_validator_quality_ratio` is problematic.
Here it brings even a bigger issue: if chunk producers don't produce chunks, chunk validators will be kicked out as well, which impacts network stability.
This is another reason to come up with the better formula.  

#### Shard assignment

As chunk producer becomes the most important role, we need to ensure that every epoch has significant amount of healthy chunk producers. 
This is a **significant difference** with current logic, where chunk-only producers generally have low stake and their performance doesn't impact overall performance. 

The most challenging part of becoming a chunk producer for a shard is to download most recent shard state within previous epoch. This is called "state sync".
Unfortunately, as of now, state sync is centralised on published snapshots, which is a major point of failure, until we don't have decentralised state sync.

Because of that, we make additional change: if node was a chunk producer for some shard in the previous epoch, and it is a chunk producer for current epoch, it will be assigned to the same shard.
This way, we minimise number of required state syncs at each epoch.

The exact algorithm needs a thorough description to satisfy different edge cases, so we will just leave a link to full explanation: https://github.com/near/nearcore/issues/11213#issuecomment-2111234940.
 
### ChunkStateWitness 

The full structure is described [here](https://github.com/near/nearcore/blob/b8f08d9ded5b7cbae9d73883785902b76e4626fc/core/primitives/src/stateless_validation.rs#L247).
Let's construct it sequentially together with explaining why every field is needed. Start from simple data:  
```rust
pub struct ChunkStateWitness {
    pub chunk_producer: AccountId,
    pub epoch_id: EpochId,
    /// The chunk header which this witness is proving.
    pub chunk_header: ShardChunkHeader,
}
```

What is needed to prove `ShardChunkHeader`?

The key function we have in codebase is [validate_chunk_with_chunk_extra_and_receipts_root](https://github.com/near/nearcore/blob/c2d80742187d9b8fc1bb672f16e3d5c144722742/chain/chain/src/validate.rs#L141). 
The main arguments there are `prev_chunk_extra: &ChunkExtra` which stands for execution result of previous chunk, and `chunk_header`.
The most important field for `ShardChunkHeader` is `prev_state_root` - consider latest implementation `ShardChunkHeaderInnerV3`. It stands for state root resulted from updating shard for the previous block, which means applying previous chunk if there is no missing chunks.
So, chunk validator needs some way to run transactions and receipts from the previous chunk. Let's call it a "main state transition" and add two more fields to state witness: 

```rust
    /// The base state and post-state-root of the main transition where we
    /// apply transactions and receipts. Corresponds to the state transition
    /// that takes us from the pre-state-root of the last new chunk of this
    /// shard to the post-state-root of that same chunk.
    pub main_state_transition: ChunkStateTransition,
    /// The transactions to apply. These must be in the correct order in which
    /// they are to be applied.
    pub transactions: Vec<SignedTransaction>,
```

where
```rust
/// Represents the base state and the expected post-state-root of a chunk's state
/// transition. The actual state transition itself is not included here.
pub struct ChunkStateTransition {
    /// The block that contains the chunk; this identifies which part of the
    /// state transition we're talking about.
    pub block_hash: CryptoHash,
    /// The partial state before the state transition. This includes whatever
    /// initial state that is necessary to compute the state transition for this
    /// chunk. It is a list of Merkle tree nodes.
    pub base_state: PartialState,
    /// The expected final state root after applying the state transition.
    pub post_state_root: CryptoHash,
}
```

Fine, but where do we take the receipts?

Receipts are internal messages, resulting from transaction execution, sent between shards, and **by default** they are not signed by anyone.

However, each receipt is an execution outcome of some transaction or other parent receipt, executed in some previous chunk.
For every chunk, we conveniently store `prev_outgoing_receipts_root` which is a Merkle hash of all receipts sent to other shards resulting by execution of this chunk. So, for every receipt, there is a proof of its generation in some parent chunk. If there are no missing chunk, then it's enough to consider chunks from previous block.

So we add another field:   

```rust
    /// Non-strict superset of the receipts that must be applied, along with
    /// information that allows these receipts to be verifiable against the 
    /// blockchain history.
    pub source_receipt_proofs: HashMap<ChunkHash, ReceiptProof>,
```

What about missing chunks though?

Unfortunately, production and inclusion of any chunk **cannot be guaranteed**:
* chunk producer may go offline;
* chunk validators may not generate 2/3 endorsements;
* block producer may not receive enough information to include chunk.

Let's handle this case as well. 
First, each chunk producer needs not just to prove main state transition, but also all state transitions for latest missing chunks:
```rust
    /// For each missing chunk after the last new chunk of the shard, we need
    /// to carry out an implicit state transition. This is technically needed
    /// to handle validator rewards distribution. This list contains one for each
    /// such chunk, in forward chronological order.
    ///
    /// After these are applied as well, we should arrive at the pre-state-root
    /// of the chunk that this witness is for.
    pub implicit_transitions: Vec<ChunkStateTransition>,
```

Then, while our shard was missing chunks, other shards could still produce chunks, which could generate receipts targeting our shards. So, we need to extend `source_receipt_proofs`. 
Field structure doesn't change, but we need to carefully pick range of set of source chunks, so different subsets will cover all source receipts without intersection.

Let's say B2 is the block that contains the last new chunk of shard S before chunk which state transition we execute, and B1 is the block that contains the last new chunk of shard S before B2. 
Then, we will define set of blocks B as the contiguous subsequence of blocks B1 (EXCLUSIVE) to B2 (inclusive) in this chunk's chain (i.e. the linear chain that this chunk's parent block is on). Lastly, source chunks are all chunks included in blocks from B.
  
The last caveat is **new** transactions introduced by chunk with `chunk_header`. As chunk header introduces `tx_root` for them, we need to check validity of this field as well.
If we don't do it, malicious chunk producer can include invalid transaction, and if it gets its chunk endorsed, nodes which track the shard must either accept invalid transaction or refuse to process chunk, but the latter means that shard will get stuck.

To validate new `tx_root`, we also need Merkle partial state to validate sender' balances, access keys, nonces, etc., which leads to two last fields to be added:
  
```rust
    pub new_transactions: Vec<SignedTransaction>,
    pub new_transactions_validation_state: PartialState,
```

The logic to produce `ChunkStateWitness` is [here](https://github.com/near/nearcore/blob/b8f08d9ded5b7cbae9d73883785902b76e4626fc/chain/client/src/stateless_validation/state_witness_producer.rs#L79). 
Itself, it requires some minor changes to the logic of applying chunks, related to generating `ChunkStateTransition::base_state`.
It is controlled by [this line](https://github.com/near/nearcore/blob/dc03a34101f77a17210873c4b5be28ef23443864/chain/chain/src/runtime/mod.rs#L977), which causes all nodes read during applying chunk to be put inside `TrieRecorder`.
After applying chunk, its contents are saved to `StateTransitionData`.

The validation logic is [here](https://github.com/near/nearcore/blob/b8f08d9ded5b7cbae9d73883785902b76e4626fc/chain/client/src/stateless_validation/chunk_validator/mod.rs#L85).
First, it performs all validation steps for which access to `ChainStore` is required, `pre_validate_chunk_state_witness` is responsible for this. It is done separately because `ChainStore` is owned by a single thread.
Then, it spawns a thread which runs computation-heavy `validate_chunk_state_witness` which main purpose is to apply chunk based on received state transitions and verify that execution results in chunk header are correct.
If validation is successful, `ChunkEndorsement` is sent.

### ChunkEndorsement

It is basically a triple of `(ChunkHash, AccountId, Signature)`.
Receiving this message means that specific chunk validator account endorsed chunk with specific chunk hash.
Ideally chunk validator would send chunk endorsement to just the next block producer at the same height for which chunk was produced.
However, block at that height can be skipped and block producers at heights h+1, h+2, ... will have to pick up the chunk.
To address that, we send `ChunkEndorsement` to all block producers at heights from h to `h+d-1`. We pick `d=5` as more than 5 skipped blocks in a row are very unlikely to occur.

On block producer side, chunk endorsements are collected and stored in `ChunkEndorsementTracker`.
Small **caveat** is that *sometimes* chunk endorsement may be received before chunk header which is required to understand that sender is indeed a validator of the chunk.
Such endorsements are stored as *pending*.
When chunk header is received, all pending endorsements are checked for validity and marked as *validated*.
All endorsements received after that are validated right away.

Finally, when block producer attempts to produce a block, in addition to checking chunk existence, it also checks that it has 2/3 endorsement stake for that chunk hash.
To make chunk inclusion verifiable, we introduce [another version](https://github.com/near/nearcore/blob/cf2caa3513f58da8be758d1c93b0900ffd5d51d2/core/primitives/src/block_body.rs#L30) of block body `BlockBodyV2` which has new field `chunk_endorsements`.
It is basically a `Vec<Vec<Option<Signature>>>` where element with indices `(s, i)` contains signature of i-th chunk validator for shard s if it was included and None otherwise.
Lastly, we add condition to block validation, such that if chunk `s` was included in the block, then block body must contain 2/3 endorsements for that shard.

This logic is triggered in `ChunkInclusionTracker` by methods [get_chunk_headers_ready_for_inclusion](https://github.com/near/nearcore/blob/6184e5dac45afb10a920cfa5532ce6b3c088deee/chain/client/src/chunk_inclusion_tracker.rs#L146) and couple similar ones. Number of ready chunks is returned by [num_chunk_headers_ready_for_inclusion](https://github.com/near/nearcore/blob/6184e5dac45afb10a920cfa5532ce6b3c088deee/chain/client/src/chunk_inclusion_tracker.rs#L178).

### Chunk validators selection

Chunk validators will be randomly assigned to validate shards, for each block (or as we may decide later, for multiple blocks in a row, if required for performance reasons). A chunk validator may be assigned multiple shards at once, if it has sufficient stake.

Each chunk validator's stake is divided into "mandates". There are full and partial mandates. The amount of stake for a full mandate is a fixed parameter determined by the stake distribution of all validators, and any remaining amount smaller than a full mandate is a partial mandate. A chunk validator therefore has zero or more full mandates plus up to one partial mandate. The list of full mandates and the list of partial mandates are then separately shuffled and partitioned equally (as in, no more than one mandate in difference between any two shards) across the shards. Any mandate assigned to a shard means that the chunk validator who owns the mandate is assigned to validate that shard. Because a chunk validator may have multiple mandates, it may be assigned multiple shards to validate.

For Stage 0, we select **target amount of mandates per shard** to 68, which was a [result of the latest research](https://near.zulipchat.com/#narrow/stream/407237-core.2Fstateless-validation/topic/validator.20seat.20assignment/near/435252304).
With this number of mandates per shard and 6 shards, we predict the protocol to be secure for 40 years at 90% confidence.
Based on target number of mandates and total chunk validators stake, [here](https://github.com/near/nearcore/blob/696190b150dd2347f9f042fa99b844b67c8001d8/core/primitives/src/validator_mandates/mod.rs#L76) we compute price of a single full mandate for each new epoch using binary search.
All the mandates are stored in new version of `EpochInfo` `EpochInfoV4` in [validator_mandates](https://github.com/near/nearcore/blob/164b7a367623eb651914eeaf1cbf3579c107c22d/core/primitives/src/epoch_manager.rs#L775) field.

After that, for each height in the epoch, [EpochInfo::sample_chunk_validators](https://github.com/near/nearcore/blob/164b7a367623eb651914eeaf1cbf3579c107c22d/core/primitives/src/epoch_manager.rs#L1224) is called to return `ChunkValidatorStakeAssignment`. It is `Vec<Vec<(ValidatorId, Balance)>>` where s-th element corresponds to s-th shard in the epoch, contains ids of all chunk validator for that height and shard, alongside with its total mandate stake assigned to that shard.
`sample_chunk_validators` basically just shuffles `validator_mandates` among shards using height-specific seed. If there are no more than 1/3 malicious validators, then by Chernoff bound the probability that at least one shard is corrupted is small enough. **This is a reason why we can split validators among shards and still rely on basic consensus assumption**. 

This way, everyone tracking block headers can compute chunk validator assignment for each height and shard. 

### Limits

`ChunkStateWitness` is relatively large message. Given large number of receivers as well, its size must be strictly limited.
If `ChunkStateWitness` for some state transition gets so uncontrollably large that it never can be handled by majority of validators, then its shard gets stuck.

All the limits are described [here](https://github.com/near/nearcore/blob/b34db1e2281fbfe1d99a36b4a90df3fc7f5d00cb/docs/misc/state_witness_size_limits.md).
Additionally, we have limit on currently stored chunk endorsements, because malicious chunk validators can spam these as well.

### Partial state witness distribution

TODO

### Protocol upgrade

The good property of the approach taken is that protocol upgrade happens almost seamlessly.

If (main transition, implicit transitions) fully belong to the protocol version before upgrade to stateless validation, chunk validator endorsements are not distributed, chunk validators are not sampled, but the protocol is safe because of all-shards tracking, as we described in "High-level flow".

If at least some transition belongs to the protocol version after upgrade, chunk header height also belongs to epoch after upgrade, so it has chunk validators assigned and requirement of 2/3 endorsements is enabled.

The minor accuracy needed is that generating and saving of state transition proofs have to be saved one epoch in advance, so we won't have to re-apply chunks to generate proofs once stateless validation is enabled. But new epoch protocol version is defined by finalization of **previous previous epoch**, so this is fine.

It also assumes that each epoch has at least two chunks, but if this is not the case, the chain is having a major disruption which never happened before.

## Security Implications

[Explicitly outline any security concerns in relation to the NEP, and potential ways to resolve or mitigate them. At the very least, well-known relevant threats must be covered, e.g. person-in-the-middle, double-spend, XSS, CSRF, etc.]



## Alternatives

[Explain any alternative designs that were considered and the rationale for not choosing them. Why your design is superior?]
TODO

## Future possibilities

* Integration with ZK allowing to get rid of large state witness distribution. If we treat state witness as a proof and ZK-ify it, anyone can validate that state witness indeed proves the new chunk header with much lower effort. Complexity of actual proof generation and computation indeed increases, but it can be distributed among chunk producers, and we can have separate concept of finality while allowing generic users to query optimistic chunks.
* TODO [Describe any natural extensions and evolutions to the NEP proposal, and how they would impact the project. Use this section as a tool to help fully consider all possible interactions with the project in your proposal. This is also a good place to "dump ideas"; if they are out of scope for the NEP but otherwise related. Note that having something written down in the future-possibilities section is not a reason to accept the current or a future NEP. Such notes should be in the section on motivation or rationale in this or subsequent NEPs. The section merely provides additional information.]

## Consequences

[This section describes the consequences, after applying the decision. All consequences should be summarized here, not just the "positive" ones. Record any concerns raised throughout the NEP discussion.]
TODO

### Positive

- p1

### Neutral

- n1

### Negative

- n1

### Backwards Compatibility

[All NEPs that introduce backwards incompatibilities must include a section describing these incompatibilities and their severity. Author must explain a proposes to deal with these incompatibilities. Submissions without a sufficient backwards compatibility treatise may be rejected outright.]

## Unresolved Issues (Optional)

[Explain any issues that warrant further discussion. Considerations

- What parts of the design do you expect to resolve through the NEP process before this gets merged?
- What parts of the design do you expect to resolve through the implementation of this feature before stabilization?
- What related issues do you consider out of scope for this NEP that could be addressed in the future independently of the solution that comes out of this NEP?]

## Changelog

[The changelog section provides historical context for how the NEP developed over time. Initial NEP submission should start with version 1.0.0, and all subsequent NEP extensions must follow [Semantic Versioning](https://semver.org/). Every version should have the benefits and concerns raised during the review. The author does not need to fill out this section for the initial draft. Instead, the assigned reviewers (Subject Matter Experts) should create the first version during the first technical review. After the final public call, the author should then finalize the last version of the decision context.]

### 1.0.0 - Initial Version

> Placeholder for the context about when and who approved this NEP version.

#### Benefits

> List of benefits filled by the Subject Matter Experts while reviewing this version:

- Benefit 1
- Benefit 2

#### Concerns

> Template for Subject Matter Experts review for this version:
> Status: New | Ongoing | Resolved

|   # | Concern | Resolution | Status |
| --: | :------ | :--------- | -----: |
|   1 |         |            |        |
|   2 |         |            |        |

## Copyright

Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
