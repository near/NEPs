---
NEP: 568
Title: Resharding V3
Authors: Adam Chudas, Aleksandr Logunov, Andrea Spurio, Marcelo Diop-Gonzalez, Shreyan Gupta, Waclaw Banasik
Status: New
DiscussionsTo: https://github.com/near/nearcore/issues/11881
Type: Protocol
Version: 1.0.0
Created: 2024-10-24
LastUpdated: 2024-10-24
---

## Summary 

This proposal introduces a new resharding implementation and shard layout for
production networks.

Resharding V3 is a significantly redesigned approach, addressing limitations of
the previous versions, [Resharding V1][NEP-040] and [Resharding V2][NEP-508].
The earlier solutions became obsolete due to major protocol changes since
Resharding V2, including the introduction of Stateless Validation, Single Shard
Tracking, and Mem-Trie.

The primary objective of Resharding V3 is to increase chain capacity by
splitting overutilized shards. A secondary aim is to lay the groundwork for
supporting Dynamic Resharding, Instant Resharding and Shard Merging in future
updates.

## Motivation

The sharded architecture of the NEAR Protocol is a cornerstone of its design, enabling parallel and distributed execution that significantly boosts overall throughput. Resharding plays a pivotal role in this system, allowing the network to adjust the number of shards to accommodate growth. By increasing the number of shards, resharding ensures the network can scale seamlessly, alleviating existing congestion, managing the rising traffic demands, and welcoming new customers. This adaptability is essential for maintaining the protocol's performance, reliability, and capacity to support a thriving, ever-expanding ecosystem.

## Specification

Resharding will be scheduled in advance by the NEAR developer team. The new
shard layout will be hardcoded into the neard binary and linked to the protocol
version. As the protocol upgrade progresses, resharding will be triggered during
the post-processing phase of the last block of the epoch. At this point, the
state of the parent shard will be split between two child shards. From the first
block of the new protocol version onward, the chain will operate with the new
shard layout.

There are two key dimensions to consider: state storage and protocol features,
along with a few additional details.

1) State Storage: Currently, the state of a shard is stored in three distinct
formats: the state, the flat state, and the mem-trie. Each of these
representations must be resharded. Logically, resharding is an almost
instantaneous event that occurs before the first block under the new shard
layout. However, in practice, some of this work may be deferred to
post-processing, as long as the chain's view reflects a fully resharded state.

1) Protocol Features: Several protocol features must integrate smoothly with the
  resharding process, including:

* Stateless Validation: Resharding must be validated and proven through
  stateless validation mechanisms.
* State Sync: Nodes must be able to sync the states of the child
  shards post-resharding.
* Cross-Shard Traffic: Receipts sent to the parent shard may need to be
  reassigned to one of the child shards.
* Receipt Handling: Delayed, postponed, buffered, and promise-yield receipts
  must be correctly distributed between the child shards.
* ShardId Semantics: The shard identifiers will become abstract identifiers
  where today they are number in the 0..num_shards range.
* Congestion Info: CongestionInfo in the chunk header would be recalculated for the child
  shards at the resharding boundary. Proof must be compatible with Stateless Validation.

### State Storage - MemTrie

MemTrie is the in-memory representation of the trie that the runtime uses for all trie accesses. This is kept in sync with the Trie representation in state.

As of today it isn't mandatory for nodes to have MemTrie feature enabled but going forward, with ReshardingV3, all nodes would require to have MemTrie enabled for resharding to happen successfully.

For the purposes of resharding, we need an efficient way to split the MemTrie into two child tries based on the boundary account. This splitting happens at the epoch boundary when the new epoch is expected to have the two child shards. The set of requirements around MemTrie splitting are:

* MemTrie splitting needs to be "instant", i.e. happen efficiently within the span of one block. The child tries need to be available for the processing of the next block in the new epoch.
* MemTrie splitting needs to be compatible with stateless validation, i.e. we need to generate a proof that the memtrie split proposed by the chunk producer is correct.
* The proof generated for splitting the MemTrie needs to be compatible with the limits of the size of state witness that we send to all chunk validators. This prevents us from doing things like iterating through all trie keys for delayed receipts etc.

With ReshardingV3 design, there's no protocol change to the structure of MemTries, however the implementation constraints required us to introduce the concept of a Frozen MemTrie. More details are in the [implementation](#state-storage---memtrie-1) section below.

Based on the requirements above, we came up with an algorithm to efficiently split the parent trie into two child tries. Trie entries can be divided into three categories based on whether the trie keys have an `account_id` prefix and based on the total number of such trie keys. Splitting of these keys is handled in different ways.

#### TrieKey with AccountID prefix

This category includes most of the trie keys like `TrieKey::Account`, `TrieKey::ContractCode`, `TrieKey::PostponedReceipt`, etc. For these keys, we can efficiently split the trie based on the boundary account trie key. Note that we only need to read all the intermediate nodes that form a part of the split key. In the example below, if "pass" is the split key, we access all the nodes along the path of `root` -> `p` -> `a` -> `s` -> `s`, while not needing to touch any of the other intermediate nodes like `o` -> `s` -> `t` in key "post". The accessed nodes form a part of the state witness as those are the only nodes that the validators would need to verify that the resharding split is correct. This limits the size of the witness to effectively O(depth) of trie for each trie key in this category.

![Splitting Trie diagram](assets/nep-0568/NEP-SplitState.png)

#### Singleton TrieKey

This category includes the trie keys `TrieKey::DelayedReceiptIndices`, `TrieKey::PromiseYieldIndices`, `TrieKey::BufferedReceiptIndices`. Notably, these are just a single entry (or O(num_shard) entries) in the trie and hence are small enough to read and modify for the children tries efficiently.

#### Indexed TrieKey

This category includes the trie keys `TrieKey::DelayedReceipt`, `TrieKey::PromiseYieldTimeout` and `TrieKey::BufferedReceipt`. The number of entries for these keys can potentially be arbitrarily large and it's not feasible to iterate through all the entries. In pre-stateless validation world, where we didn't care about state witness size limits, for ReshardingV2 we could just iterate over all delayed receipts and split them into the respective child shards.

For ReshardingV3, these are handled by either of the two strategies

* `TrieKey::DelayedReceipt` and `TrieKey::PromiseYieldTimeout` are handled by duplicating entries across both child shards as each entry could belong to either of the child shards. More details in the [Delayed Receipts](#delayed-receipt-handling) and [Promise Yield](#promiseyield-receipt-handling) sections below.

* `TrieKey::BufferedReceipt` are independent of the account_id and therefore can be sent to either of the child shards, but not both. We copy the buffered receipts and the associated metadata to the child shard with the lower index. More details in the [Buffered Receipts](#buffered-receipt-handling) section below.

### State Storage - Flat State

Flat State is a collection of key-value pairs stored on disk and each entry
contains a reference to its ShardId. When splitting a shard, every item inside
its Flat State must be correctly reassigned to either one of the new children;
due to technical limitations such operation can't be completed instantaneously.

Flat State main purposes are allowing the creation of State Sync snapshots and
the construction of Mem Tries. Fortunately, these two operations can be delayed
until resharding is completed. Note also that with Mem Tries enabled the chain
can move forward even if the current status of Flat State is not in sync with
the latest block. 

For the reason stated above, the chosen strategy is to reshard Flat State in a
long-running background task. The new shards' states must converge with their
Mem Tries representation in a reasonable amount of time.

Splitting a shard's Flat State is performed in multiple steps:

1) A post-processing 'split' task is created during the last block of the old
   shard layout, instantaneously. 
2) The 'split' task runs in parallel with the chain for a certain amount of
   time. Inside this routine every key-value pair belonging to the shard being
   split (also called parent shard) is copied into either the left or the right
   child Flat State. Entries linked to receipts are handled in a special way.
3) Once the task is completed, the parent shard Flat State is cleaned up. The
   children shards Flat States have their state in sync with last block of the
   old shard layout.
4) Children shards must apply the delta changes from the first block of the new
   shard layout until the final block of the canonical chain. This operation is
   done in another background task to avoid slowdowns while processing blocks.
5) Children shards Flat States are now ready and can be used to take State Sync
   snapshots and to reload Mem Tries.

### State Storage - State

// TODO Describe integration with cold storage once design is ready

Each shardâ€™s Trie is stored in the `State` column of the database, with keys prefixed by `ShardUId`, followed by a node's hash.
This structure uniquely identifies each shardâ€™s data. To avoid copying all entries under a new `ShardUId` during resharding,
a mapping strategy allows child shards to access ancestor shard data without directly creating new entries.

A naive approach to resharding would involve copying all `State` entries with a new `ShardUId` for a child shard, effectively duplicating the state.
This method, while straightforward, is not feasible because copying a large state would take too much time.
Resharding needs to appear complete between two blocks, so a direct copy would not allow the process to occur quickly enough.

To address this, Resharding V3 employs an efficient mapping strategy, using the `DBCol::ShardUIdMapping` column
to link each child shardâ€™s `ShardUId` to the closest ancestorâ€™s `ShardUId` holding the relevant data.
This allows child shards to access and update state data under the ancestor shardâ€™s prefix without duplicating entries.

Initially, `ShardUIdMapping` is empty, as existing shards map to themselves. During resharding, a mapping entry is added to `ShardUIdMapping`,
pointing each child shardâ€™s `ShardUId` to the appropriate ancestor. Mappings persist as long as any descendant shard references the ancestorâ€™s data.
Once a node stops tracking all children and descendants of a shard, the entry for that shard can be removed, allowing its data to be garbage collected.
For archival nodes, mappings are retained indefinitely to maintain access to the full historical state.

This mapping strategy enables efficient shard management during resharding events,
supporting smooth transitions without altering storage structures directly.


### Stateless Validation

### State Sync

### Cross Shard Traffic

### Delayed Receipt Handling

### PromiseYield Receipt Handling

### Buffered Receipt Handling

### ShardId Semantics

Currently, shard IDs are represented as numbers within the range `[0,n)`, where n is the total number of shards. These shard IDs are sorted in the same order as the account ID ranges assigned to them. While this approach is straightforward, it complicates resharding operations, particularly when splitting a shard in the middle of the range. Such a split requires reindexing all subsequent shards with higher IDs, adding complexity to the process.

In this NEP, we propose updating the ShardId semantics to allow for arbitrary identifiers. Although ShardIds will remain integers, they will no longer be restricted to the `[0,n)` range, and they may appear in any order. The only requirement is that each ShardId must be unique. In practice, during resharding, the ID of a parent shard will be removed from the ShardLayout, and the new child shards will be assigned unique IDs - `max(shard_ids)+1` and `max(shard_ids)+2`.

## Reference Implementation

```text
[This technical section is required for Protocol proposals but optional for other categories. A draft implementation should demonstrate a minimal implementation that assists in understanding or implementing this proposal. Explain the design in sufficient detail that:

* Its interaction with other features is clear.
* Where possible, include a Minimum Viable Interface subsection expressing the required behavior and types in a target programming language. (ie. traits and structs for rust, interfaces and classes for javascript, function signatures and structs for c, etc.)
* It is reasonably clear how the feature would be implemented.
* Corner cases are dissected by example.
* For protocol changes: A link to a draft PR on nearcore that shows how it can be integrated in the current code. It should at least solve the key technical challenges.

The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.]
```

### State Storage - MemTrie

The current implementation of MemTrie uses a pool of memory (`STArena`) to allocate and deallocate nodes and internal pointers in this pool to reference child nodes. MemTries, unlike the State representation of Trie, do not work with the hash of the nodes but internal memory pointers directly. Additionally, MemTries are not thread safe and one MemTrie exists per shard.

As described in [MemTrie](#state-storage---memtrie) section above, we need an efficient way to split the MemTrie into two child MemTries within a span of 1 block. What makes this challenging is that the current implementation of MemTrie is not thread safe and can not be shared across two shards.

The naive way to create two MemTries for the child shards would be to iterate through all the entries of the parent MemTrie and fill in these values into the child MemTries. This however is prohibitively time consuming.

The solution to this problem was to introduce the concept of Frozen MemTrie (with a `FrozenArena`) which is a cloneable, read-only, thread-safe snapshot of a MemTrie. We can call the `freeze` method on an existing MemTrie that converts it into a Frozen MemTrie. Note that this process consumes the original MemTrie and we can no longer allocate and deallocate nodes to it.

Along with `FrozenArena`, we also introduce a `HybridArena` which is effectively a base made of `FrozenArena` with a top layer of `STArena` where we support allocating and deallocating new nodes into the MemTrie. Newly allocated nodes can reference/point to nodes in the `FrozenArena`. We use this Hybrid MemTrie as a temporary MemTrie while the flat storage is being constructed in the background.

While Frozen MemTries provide the benefits of being compatible with instant resharding, they come at the cost of memory consumption. Once a MemTrie is frozen, since it doesn't support deallocation of memory, it continues to consume as much memory as it did at the time of freezing. In case a node is tracking only one of the child shards, a Frozen MemTrie would continue to use the same amount of memory as the parent trie. Due to this, Hybrid MemTries are only a temporary solution and we rebuild the MemTrie for the children once the post-processing step for Flat Storage is completed.

Additionally, a node would have to support 2x the memory footprint of a single trie as after resharding, we would have two copies of the trie in memory, one from the temporary Hybrid MemTrie in use for block production, and other from the background MemTrie that would be under construction. Once the background MemTrie is fully constructed and caught up with the latest block, we do an in-place swap of the Hybrid MemTrie with the new child MemTrie and deallocate the memory from the Hybrid MemTrie.

During a resharding event, at the boundary of the epoch, when we need to split the parent shard into the two child shards, we do the following steps:

1. Freeze the parent MemTrie arena to create a read-only frozen arena that represents a snapshot of the state as of the time of freezing, i.e. after postprocessing last block of epoch. Note that we no longer require the parent MemTrie in runtime going forward.
2. We cheaply clone the Frozen MemTrie for both the child MemTries to use. Note that this doesn't clone the parent arena memory, but just increases the refcount.
3. We then create a new MemTrie with HybridArena for each of the children. The base of the MemTrie is the read-only FrozenArena while all new node allocations happens on a dedicated STArena memory pool for each child MemTrie. This is the temporary MemTrie that we use while Flat Storage is being built in the background.
4. Once the Flat Storage is constructed in the post processing step of resharding, we use that to load a new MemTrie and catchup to the latest block.
5. After the new child MemTrie has caught up to the latest block, we do an in-place swap in Client and discard the Hybrid MemTrie.

![Hybrid MemTrie diagram](assets/nep-0568/NEP-HybridMemTrie.png)

### State Storage - Flat State

Resharding Flat State is a time consuming operation and it runs in parallel with block processing for several block heights.
Thus, there are a few important aspects to consider during implementation:

* Flat State's own status should be resilient to application crashes.
* The parent shard's Flat State should be split at the correct block height.
* New shards' Flat States should eventually converge to same representation the chain is using to process blocks (MemTries).
* Resharding should work correctly in the presence of chain forks.
* Retired shards are cleaned up.

Note that the Flat States of the newly created shards won't be available until resharding is completed. This is fine because the temporary MemTries are
built instantly and they can satisfy all block processing needs.

The main component responsible to carry out resharding on Flat State is the [FlatStorageResharder](https://github.com/near/nearcore/blob/f4e9dd5d6e07089dfc789221ded8ec83bfe5f6e8/chain/chain/src/flat_storage_resharder.rs#L68).

#### Flat State's status persistence

Every shard Flat State has a status associated to it and stored in the database, called `FlatStorageStatus`. We propose to extend the existing object
by adding the new enum variant named `FlatStorageStatus::Resharding`. This approach has two benefits. First, the progress of any Flat State resharding is
persisted to disk, which makes the operation resilient to a node crash or restart. Second, resuming resharding on node restart shares the same code path as Flat
State creation (see `FlatStorageShardCreator`), reducing the code duplication factor.

`FlatStorageStatus` is updated at every committable step of resharding. The commit points are the following:

* Beginning of resharding or, in other words, the last block of the old shard layout.
* Scheduling of the _"split parent shard"_ task.
* Execution, cancellation or failure of the _"split parent shard"_ task.
* Execution or failure of any _"child catchup"_ task.

#### Splitting a shard Flat State

When, at the end of an epoch, the shard layout changes we identify a so called _resharding block_ that corresponds to the last block of the current epoch.
A task to split the parent shard's Flat State is scheduled to happen after the _resharding block_ becomes final. The reason to wait for the finality condition
is to avoid a split on a block that might be excluded from the canonical chain; needless to say, such situation would lock the node
into an erroneous state.

Inside the split task we iterate over the Flat State and copy each element into either child. This routine is performed in batches in order to lessen the performance 
impact on the node.

Finally, if the split completes successfully, the parent shard Flat State is removed from the database and the children Flat States enter a catch-up phase.

One current technical limitation is that, upon a node crash or restart, the _"split parent shard"_ task will start copying all elements again from the beginning.

A reference implementation of splitting a Flat State can be found in [FlatStorageResharder::split_shard_task](https://github.com/near/nearcore/blob/fecce019f0355cf89b63b066ca206a3cdbbdffff/chain/chain/src/flat_storage_resharder.rs#L295).

#### Values assignment from parent to child shards

Key-value pairs in the parent shard Flat State are inherited by children according to the rules stated below.

Elements inherited by the child shard which tracks the `account_id` contained in the key:

* `ACCOUNT`
* `CONTRACT_DATA`
* `CONTRACT_CODE`
* `ACCESS_KEY`
* `RECEIVED_DATA`
* `POSTPONED_RECEIPT_ID`
* `PENDING_DATA_COUNT`
* `POSTPONED_RECEIPT`
* `PROMISE_YIELD_RECEIPT`

Elements inherited by both children:

* `DELAYED_RECEIPT_OR_INDICES`
* `PROMISE_YIELD_INDICES`
* `PROMISE_YIELD_TIMEOUT`
* `BANDWIDTH_SCHEDULER_STATE`

Elements inherited only be the lowest index child:

* `BUFFERED_RECEIPT_INDICES `
* `BUFFERED_RECEIPT`

#### Bring children shards up to date with the chain's head

Children shards' Flat States build a complete view of their content at the height of the `resharding block` sometime during the new epoch 
after resharding. At that point in time many new blocks have been processed already, and these will most likely contain updates for the new shards. A catch-up step is necessary to apply all Flat State deltas accumulated until now.

This phase of resharding doesn't have to take extra steps to handle chain forks. On one hand, the catch-up task doesn't start until the parent shard 
splitting is done, and at such point we know the `resharding block` is final; on the other hand, Flat State deltas are capable of handling forks automatically.

The catch-up task commits to the database "batches" of Flat State deltas. If the application crashes or restarts the task will resume from where it left.

Once all Flat State deltas are applied, the child shard's status is changed to `Ready` and clean up of Flat State deltas leftovers is performed. 

A reference implementation of the catch-up task can be found in [FlatStorageResharder::shard_catchup_task](https://github.com/near/nearcore/blob/fecce019f0355cf89b63b066ca206a3cdbbdffff/chain/chain/src/flat_storage_resharder.rs#L564).

#### Failure of Flat State resharding

In the current proposal any failure during Flat State resharding is considered non-recoverable. 
`neard` will attempt resharding again on restart, but no automatic recovery is implemented.

### State Storage - State mapping

To enable efficient shard state management during resharding, Resharding V3 uses the `DBCol::ShardUIdMapping` column.
This mapping allows child shards to reference ancestor shard data, avoiding the need for immediate duplication of state entries. 

#### Mapping application in adapters

The core of the mapping logic is applied in `TrieStoreAdapter` and `TrieStoreUpdateAdapter`, which act as layers over the general `Store` interface.
Hereâ€™s a breakdown of the key functions involved:

* **Key resolution**:
  The `get_key_from_shard_uid_and_hash` function is central to determining the correct `ShardUId` for state access.
  At a high level, operations use the child shard's `ShardUId`, but within this function,
  the `DBCol::ShardUIdMapping` column is checked to determine if an ancestor `ShardUId` should be used instead.

  ```rust
  fn get_key_from_shard_uid_and_hash(
      store: &Store,
      shard_uid: ShardUId,
      hash: &CryptoHash,
  ) -> [u8; 40] {
      let mapped_shard_uid = store
          .get_ser::<ShardUId>(DBCol::StateShardUIdMapping, &shard_uid.to_bytes())
          .expect("get_key_from_shard_uid_and_hash() failed")
          .unwrap_or(shard_uid);
      let mut key = [0; 40];
      key[0..8].copy_from_slice(&mapped_shard_uid.to_bytes());
      key[8..].copy_from_slice(hash.as_ref());
      key
  }
  ```

  This function first attempts to retrieve a mapped ancestor `ShardUId` from `DBCol::ShardUIdMapping`.
  If no mapping exists, it defaults to the provided child `ShardUId`.
  This resolved `ShardUId` is then combined with the `node_hash` to form the final key used in `State` column operations.

* **State access operations**:
  The `TrieStoreAdapter` and `TrieStoreUpdateAdapter` use `get_key_from_shard_uid_and_hash` to correctly resolve the key for both reads and writes.
  Example methods include:

   ```rust
   // In TrieStoreAdapter
   pub fn get(&self, shard_uid: ShardUId, hash: &CryptoHash) -> Result<Arc<[u8]>, StorageError> {
      let key = get_key_from_shard_uid_and_hash(self.store, shard_uid, hash);
      self.store.get(DBCol::State, &key)
   }

   // In TrieStoreUpdateAdapter
   pub fn increment_refcount_by(
      &mut self,
      shard_uid: ShardUId,
      hash: &CryptoHash,
      data: &[u8],
      increment: NonZero<u32>,
   ) {
      let key = get_key_from_shard_uid_and_hash(self.store, shard_uid, hash);
      self.store_update.increment_refcount_by(DBCol::State, key.as_ref(), data, increment);
   }
   ```

  The `get` function retrieves data using the resolved `ShardUId` and key, while `increment_refcount_by` manages reference counts,
  ensuring correct tracking even when accessing data under an ancestor shard.

#### Mapping retention and cleanup

Mappings in `DBCol::ShardUIdMapping` persist as long as any descendant relies on an ancestorâ€™s data.
To manage this, the `set_shard_uid_mapping` function in `TrieStoreUpdateAdapter` adds a new mapping during resharding:

```rust
fn set_shard_uid_mapping(&mut self, child_shard_uid: ShardUId, parent_shard_uid: ShardUId) {
    self.store_update.set(
        DBCol::StateShardUIdMapping,
        child_shard_uid.to_bytes().as_ref(),
        &borsh::to_vec(&parent_shard_uid).expect("Borsh serialize cannot fail"),
    )
}
```

When a node stops tracking all descendants of a shard, the associated mapping entry can be removed, allowing RocksDB to perform garbage collection.
For archival nodes, mappings are retained permanently to ensure access to the historical state of all shards.

This implementation ensures efficient and scalable shard state transitions,
allowing child shards to use ancestor data without creating redundant entries.

## Security Implications

```text
[Explicitly outline any security concerns in relation to the NEP, and potential ways to resolve or mitigate them. At the very least, well-known relevant threats must be covered, e.g. person-in-the-middle, double-spend, XSS, CSRF, etc.]
```

## Alternatives

```text
[Explain any alternative designs that were considered and the rationale for not choosing them. Why your design is superior?]
```

## Future possibilities

* Dynamic Resharding - In this proposal, resharding is scheduled in advance and hardcoded within the neard binary. In the future, we aim to enable the chain to dynamically trigger and execute resharding autonomously, allowing it to adjust capacity automatically based on demand.
* Fast Dynamic Resharding - In the Dynamic Resharding extension, the new shard layout is configured for the second upcoming epoch. This means that a full epoch must pass before the chain transitions to the updated shard layout. In the future, our goal is to accelerate this process by finalizing the previous epoch more quickly, allowing the chain to adopt the new layout as soon as possible.
* Shard Merging - In this proposal the only allowed resharding operation is shard splitting. In the future, we aim to enable shard merging, allowing underutilized shards to be combined with neighboring shards. This would allow the chain to free up resources and reallocate them where they are most needed.

## Consequences

```text
[This section describes the consequences, after applying the decision. All consequences should be summarized here, not just the "positive" ones. Record any concerns raised throughout the NEP discussion.]
```

### Positive

* p1

### Neutral

* n1

### Negative

* n1

### Backwards Compatibility

```text
[All NEPs that introduce backwards incompatibilities must include a section describing these incompatibilities and their severity. Author must explain a proposes to deal with these incompatibilities. Submissions without a sufficient backwards compatibility treatise may be rejected outright.]
```

## Unresolved Issues (Optional)

```text
[Explain any issues that warrant further discussion. Considerations

* What parts of the design do you expect to resolve through the NEP process before this gets merged?
* What parts of the design do you expect to resolve through the implementation of this feature before stabilization?
* What related issues do you consider out of scope for this NEP that could be addressed in the future independently of the solution that comes out of this NEP?]
```

## Changelog

```text
[The changelog section provides historical context for how the NEP developed over time. Initial NEP submission should start with version 1.0.0, and all subsequent NEP extensions must follow [Semantic Versioning](https://semver.org/). Every version should have the benefits and concerns raised during the review. The author does not need to fill out this section for the initial draft. Instead, the assigned reviewers (Subject Matter Experts) should create the first version during the first technical review. After the final public call, the author should then finalize the last version of the decision context.]
```

### 1.0.0 - Initial Version

> Placeholder for the context about when and who approved this NEP version.

#### Benefits

> List of benefits filled by the Subject Matter Experts while reviewing this version:

* Benefit 1
* Benefit 2

#### Concerns

> Template for Subject Matter Experts review for this version:
> Status: New | Ongoing | Resolved

|    # | Concern | Resolution | Status |
| ---: | :------ | :--------- | -----: |
|    1 |         |            |        |
|    2 |         |            |        |

## Copyright

Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).


<!-- links --> 

[NEP-040]: https://github.com/near/NEPs/blob/master/specs/Proposals/0040-split-states.md
[NEP-508]: https://github.com/near/NEPs/blob/master/neps/nep-0508.md
